{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 860001/860001 [25:18<00:00, 566.39it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COCO-2017 dataset has been successfully converted to TensorFlow ImageFolder dataset.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Paths\n",
    "coco_path = r'C:\\\\Users\\\\xxnab\\\\fiftyone\\\\coco-2017\\\\train'\n",
    "output_path = r'C:\\\\Users\\\\xxnab\\\\fiftyone\\\\coco-2017\\\\tensorflow_dataset'\n",
    "\n",
    "# Load annotations\n",
    "with open(os.path.join(coco_path, 'labels.json')) as f:\n",
    "    annotations = json.load(f)\n",
    "\n",
    "# Create class directories\n",
    "categories = {cat['id']: cat['name'] for cat in annotations['categories']}\n",
    "for cat in categories.values():\n",
    "    os.makedirs(os.path.join(output_path, 'train', cat), exist_ok=True)\n",
    "\n",
    "# Move images to class directories\n",
    "for ann in tqdm(annotations['annotations']):\n",
    "    img_id = ann['image_id']\n",
    "    cat_id = ann['category_id']\n",
    "    img_name = f'{img_id:012d}.jpg'\n",
    "    src_path = os.path.join(coco_path, 'data', img_name)\n",
    "    dst_path = os.path.join(output_path, 'train', categories[cat_id], img_name)\n",
    "    shutil.copy(src_path, dst_path)\n",
    "\n",
    "print(\"COCO-2017 dataset has been successfully converted to TensorFlow ImageFolder dataset.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Moving train images: 100%|██████████| 602000/602000 [26:44<00:00, 375.28it/s]  \n",
      "Moving validation images: 100%|██████████| 129000/129000 [06:09<00:00, 348.85it/s]\n",
      "Moving test images: 100%|██████████| 129001/129001 [06:06<00:00, 351.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COCO-2017 dataset has been successfully split into train, validation, and test sets.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "import json\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Paths\n",
    "coco_path = r'C:\\\\Users\\\\xxnab\\\\fiftyone\\\\coco-2017\\\\train'\n",
    "output_path = r'C:\\\\Users\\\\xxnab\\\\fiftyone\\\\coco-2017\\\\tensorflow_dataset'\n",
    "\n",
    "# Load annotations\n",
    "with open(os.path.join(coco_path, 'labels.json')) as f:\n",
    "    annotations = json.load(f)\n",
    "\n",
    "# Create class directories for train, validation, and test\n",
    "categories = {cat['id']: cat['name'] for cat in annotations['categories']}\n",
    "for split in ['train', 'validation', 'test']:\n",
    "    for cat in categories.values():\n",
    "        os.makedirs(os.path.join(output_path, split, cat), exist_ok=True)\n",
    "\n",
    "# Split ratios\n",
    "train_ratio = 0.7\n",
    "val_ratio = 0.15\n",
    "test_ratio = 0.15\n",
    "\n",
    "# Shuffle and split the annotations\n",
    "random.shuffle(annotations['annotations'])\n",
    "total_annotations = len(annotations['annotations'])\n",
    "train_end = int(train_ratio * total_annotations)\n",
    "val_end = int((train_ratio + val_ratio) * total_annotations)\n",
    "\n",
    "train_annotations = annotations['annotations'][:train_end]\n",
    "val_annotations = annotations['annotations'][train_end:val_end]\n",
    "test_annotations = annotations['annotations'][val_end:]\n",
    "\n",
    "# Function to move images to the respective directories\n",
    "def move_images(annotations, split):\n",
    "    for ann in tqdm(annotations, desc=f'Moving {split} images'):\n",
    "        img_id = ann['image_id']\n",
    "        cat_id = ann['category_id']\n",
    "        img_name = f'{img_id:012d}.jpg'\n",
    "        src_path = os.path.join(coco_path, 'data', img_name)\n",
    "        dst_path = os.path.join(output_path, split, categories[cat_id], img_name)\n",
    "        shutil.copy(src_path, dst_path)\n",
    "\n",
    "# Move images to the respective directories\n",
    "move_images(train_annotations, 'train')\n",
    "move_images(val_annotations, 'validation')\n",
    "move_images(test_annotations, 'test')\n",
    "\n",
    "print(\"COCO-2017 dataset has been successfully split into train, validation, and test sets.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Person only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Moving train_person images: 100%|██████████| 183725/183725 [08:05<00:00, 378.54it/s]\n",
      "Moving validation_person images: 100%|██████████| 39370/39370 [02:10<00:00, 301.73it/s]\n",
      "Moving test_person images: 100%|██████████| 39370/39370 [01:37<00:00, 402.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COCO-2017 dataset has been successfully split into train, validation, and test sets for the 'person' class.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "import json\n",
    "import random\n",
    "import logging\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Paths\n",
    "coco_path = r'C:\\\\Users\\\\xxnab\\\\fiftyone\\\\coco-2017\\\\train'\n",
    "output_path = r'C:\\\\Users\\\\xxnab\\\\fiftyone\\\\coco-2017\\\\tensorflow_dataset'\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(filename='dataset_conversion.log', level=logging.INFO)\n",
    "\n",
    "# Load annotations\n",
    "with open(os.path.join(coco_path, 'labels.json')) as f:\n",
    "    annotations = json.load(f)\n",
    "\n",
    "# Filter annotations for 'person' class\n",
    "person_id = next(cat['id'] for cat in annotations['categories'] if cat['name'] == 'person')\n",
    "person_annotations = [ann for ann in annotations['annotations'] if ann['category_id'] == person_id]\n",
    "\n",
    "# Create directories for train, validation, and test\n",
    "for split in ['train_person', 'validation_person', 'test_person']:\n",
    "    os.makedirs(os.path.join(output_path, split, 'person'), exist_ok=True)\n",
    "\n",
    "# Split ratios\n",
    "train_ratio = 0.7\n",
    "val_ratio = 0.15\n",
    "test_ratio = 0.15\n",
    "\n",
    "# Shuffle and split the annotations\n",
    "random.shuffle(person_annotations)\n",
    "total_annotations = len(person_annotations)\n",
    "train_end = int(train_ratio * total_annotations)\n",
    "val_end = int((train_ratio + val_ratio) * total_annotations)\n",
    "\n",
    "train_annotations = person_annotations[:train_end]\n",
    "val_annotations = person_annotations[train_end:val_end]\n",
    "test_annotations = person_annotations[val_end:]\n",
    "\n",
    "# Function to move images to the respective directories\n",
    "def move_images(annotations, split):\n",
    "    for ann in tqdm(annotations, desc=f'Moving {split} images'):\n",
    "        img_id = ann['image_id']\n",
    "        img_name = f'{img_id:012d}.jpg'\n",
    "        src_path = os.path.join(coco_path, 'data', img_name)\n",
    "        dst_path = os.path.join(output_path, split, 'person', img_name)\n",
    "        \n",
    "        try:\n",
    "            shutil.copy(src_path, dst_path)\n",
    "        except FileNotFoundError:\n",
    "            logging.warning(f\"File {img_name} not found. Skipping.\")\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error copying {img_name}: {e}\")\n",
    "\n",
    "# Move images to the respective directories\n",
    "move_images(train_annotations, 'train_person')\n",
    "move_images(val_annotations, 'validation_person')\n",
    "move_images(test_annotations, 'test_person')\n",
    "\n",
    "print(\"COCO-2017 dataset has been successfully split into train, validation, and test sets for the 'person' class.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "person plus negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Moving train images: 100%|██████████| 602000/602000 [19:32<00:00, 513.50it/s] \n",
      "Moving validation images: 100%|██████████| 129000/129000 [05:18<00:00, 404.91it/s]\n",
      "Moving test images: 100%|██████████| 129001/129001 [05:22<00:00, 400.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset has been successfully split and filtered.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "import json\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Paths\n",
    "coco_path = r'C:\\\\Users\\\\xxnab\\\\fiftyone\\\\coco-2017\\\\train'\n",
    "output_path = r'C:\\\\Users\\\\xxnab\\\\fiftyone\\\\coco-2017\\\\tensorflow_dataset'\n",
    "\n",
    "# Load annotations\n",
    "with open(os.path.join(coco_path, 'labels.json')) as f:\n",
    "    annotations = json.load(f)\n",
    "\n",
    "# Create directories\n",
    "for split in ['train', 'validation', 'test']:\n",
    "    os.makedirs(os.path.join(output_path, split), exist_ok=True)\n",
    "    os.makedirs(os.path.join(output_path, f'{split}_person', 'person'), exist_ok=True)\n",
    "    os.makedirs(os.path.join(output_path, f'{split}_person', 'negative'), exist_ok=True)\n",
    "\n",
    "# Split ratios\n",
    "train_ratio = 0.7\n",
    "val_ratio = 0.15\n",
    "test_ratio = 0.15\n",
    "\n",
    "# Shuffle and split the annotations\n",
    "random.shuffle(annotations['annotations'])\n",
    "total_annotations = len(annotations['annotations'])\n",
    "train_end = int(train_ratio * total_annotations)\n",
    "val_end = int((train_ratio + val_ratio) * total_annotations)\n",
    "\n",
    "train_annotations = annotations['annotations'][:train_end]\n",
    "val_annotations = annotations['annotations'][train_end:val_end]\n",
    "test_annotations = annotations['annotations'][val_end:]\n",
    "\n",
    "# Function to move images\n",
    "def move_images(annotations, split):\n",
    "    person_images = set()\n",
    "    all_images = set()\n",
    "\n",
    "    for ann in tqdm(annotations, desc=f'Moving {split} images'):\n",
    "        img_id = ann['image_id']\n",
    "        cat_id = ann['category_id']\n",
    "        img_name = f'{img_id:012d}.jpg'\n",
    "        src_path = os.path.join(coco_path, 'data', img_name)\n",
    "        \n",
    "        all_images.add(img_name)\n",
    "        if cat_id == 1:  # Assuming 'person' class has ID 1\n",
    "            person_images.add(img_name)\n",
    "            dst_path = os.path.join(output_path, f'{split}_person', 'person', img_name)\n",
    "        else:\n",
    "            dst_path = os.path.join(output_path, f'{split}_person', 'negative', img_name)\n",
    "        \n",
    "        if os.path.exists(src_path):\n",
    "            shutil.copy(src_path, dst_path)\n",
    "        else:\n",
    "            print(f\"Warning: {src_path} does not exist and will be skipped.\")\n",
    "\n",
    "    # Move negative samples\n",
    "    for img_name in all_images - person_images:\n",
    "        src_path = os.path.join(coco_path, 'data', img_name)\n",
    "        dst_path = os.path.join(output_path, f'{split}_person', 'negative', img_name)\n",
    "        if os.path.exists(src_path):\n",
    "            shutil.copy(src_path, dst_path)\n",
    "        else:\n",
    "            print(f\"Warning: {src_path} does not exist and will be skipped.\")\n",
    "\n",
    "# Move images for each split\n",
    "move_images(train_annotations, 'train')\n",
    "move_images(val_annotations, 'validation')\n",
    "move_images(test_annotations, 'test')\n",
    "\n",
    "print(\"Dataset has been successfully split and filtered.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model training with 80 classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 342996 images belonging to 80 classes.\n",
      "Found 95879 images belonging to 80 classes.\n",
      "Found 95894 images belonging to 80 classes.\n",
      "Epoch 1/10\n",
      "\u001b[1m10719/10719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4586s\u001b[0m 427ms/step - accuracy: 0.2625 - loss: 2.6706 - val_accuracy: 0.3151 - val_loss: 2.3428\n",
      "Epoch 2/10\n",
      "\u001b[1m10719/10719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4029s\u001b[0m 376ms/step - accuracy: 0.2814 - loss: 2.4755 - val_accuracy: 0.3331 - val_loss: 2.2944\n",
      "Epoch 3/10\n",
      "\u001b[1m10719/10719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3946s\u001b[0m 368ms/step - accuracy: 0.2854 - loss: 2.4425 - val_accuracy: 0.3210 - val_loss: 2.2690\n",
      "Epoch 4/10\n",
      "\u001b[1m10719/10719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4532s\u001b[0m 423ms/step - accuracy: 0.2863 - loss: 2.4228 - val_accuracy: 0.3270 - val_loss: 2.2444\n",
      "Epoch 5/10\n",
      "\u001b[1m10719/10719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4054s\u001b[0m 378ms/step - accuracy: 0.2861 - loss: 2.4039 - val_accuracy: 0.3289 - val_loss: 2.2410\n",
      "\u001b[1m2997/2997\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1246s\u001b[0m 416ms/step - accuracy: 0.3328 - loss: 2.2903\n",
      "Test Loss: 2.2829558849334717\n",
      "Test Accuracy: 0.3355267345905304\n",
      "\u001b[1m2997/2997\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m887s\u001b[0m 296ms/step\n",
      "Classification Report\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "      airplane       0.01      0.02      0.01       712\n",
      "         apple       0.01      0.00      0.01       611\n",
      "      backpack       0.00      0.00      0.00      1189\n",
      "        banana       0.01      0.01      0.01       835\n",
      "  baseball bat       0.00      0.00      0.00       444\n",
      "baseball glove       0.00      0.00      0.00       494\n",
      "          bear       0.00      0.00      0.00       184\n",
      "           bed       0.00      0.01      0.00       595\n",
      "         bench       0.01      0.00      0.00      1216\n",
      "       bicycle       0.00      0.00      0.00       843\n",
      "          bird       0.01      0.01      0.01      1058\n",
      "          boat       0.02      0.02      0.02      1067\n",
      "          book       0.01      0.00      0.00      2258\n",
      "        bottle       0.03      0.01      0.02      2730\n",
      "          bowl       0.02      0.01      0.02      1804\n",
      "      broccoli       0.01      0.01      0.01       777\n",
      "           bus       0.00      0.00      0.00       823\n",
      "          cake       0.01      0.00      0.00       716\n",
      "           car       0.06      0.02      0.03      4667\n",
      "        carrot       0.02      0.01      0.01       787\n",
      "           cat       0.01      0.01      0.01       658\n",
      "    cell phone       0.01      0.00      0.00       897\n",
      "         chair       0.04      0.04      0.04      4157\n",
      "         clock       0.01      0.00      0.01       878\n",
      "         couch       0.00      0.00      0.00       842\n",
      "           cow       0.01      0.01      0.01       815\n",
      "           cup       0.02      0.01      0.01      2508\n",
      "  dining table       0.02      0.04      0.03      2176\n",
      "           dog       0.01      0.01      0.01       793\n",
      "         donut       0.00      0.01      0.00       651\n",
      "      elephant       0.01      0.01      0.01       629\n",
      "  fire hydrant       0.01      0.01      0.01       284\n",
      "          fork       0.01      0.00      0.00       770\n",
      "       frisbee       0.00      0.00      0.00       374\n",
      "       giraffe       0.00      0.01      0.01       660\n",
      "    hair drier       0.00      0.00      0.00        33\n",
      "       handbag       0.00      0.00      0.00      1568\n",
      "         horse       0.01      0.00      0.00       844\n",
      "       hot dog       0.00      0.00      0.00       327\n",
      "      keyboard       0.00      0.00      0.00       405\n",
      "          kite       0.01      0.00      0.00       856\n",
      "         knife       0.01      0.01      0.01      1017\n",
      "        laptop       0.01      0.01      0.01       663\n",
      "     microwave       0.00      0.00      0.00       267\n",
      "    motorcycle       0.01      0.02      0.01      1014\n",
      "         mouse       0.01      0.01      0.01       318\n",
      "        orange       0.00      0.00      0.00       634\n",
      "          oven       0.01      0.01      0.01       502\n",
      " parking meter       0.00      0.00      0.00       162\n",
      "        person       0.26      0.47      0.34     25124\n",
      "         pizza       0.01      0.01      0.01       723\n",
      "  potted plant       0.00      0.00      0.00      1073\n",
      "  refrigerator       0.00      0.01      0.01       370\n",
      "        remote       0.01      0.01      0.01       743\n",
      "      sandwich       0.01      0.01      0.01       550\n",
      "      scissors       0.00      0.00      0.00       188\n",
      "         sheep       0.01      0.01      0.01       805\n",
      "          sink       0.01      0.01      0.01       847\n",
      "    skateboard       0.01      0.01      0.01       750\n",
      "          skis       0.00      0.00      0.00       839\n",
      "     snowboard       0.00      0.00      0.00       347\n",
      "         spoon       0.02      0.00      0.00       855\n",
      "   sports ball       0.02      0.00      0.00       829\n",
      "     stop sign       0.00      0.00      0.00       260\n",
      "      suitcase       0.02      0.01      0.01       684\n",
      "     surfboard       0.00      0.00      0.00       789\n",
      "    teddy bear       0.00      0.00      0.00       520\n",
      " tennis racket       0.01      0.01      0.01       675\n",
      "           tie       0.00      0.00      0.00       814\n",
      "       toaster       0.00      0.00      0.00        30\n",
      "        toilet       0.00      0.00      0.00       561\n",
      "    toothbrush       0.00      0.00      0.00       269\n",
      " traffic light       0.02      0.01      0.02      1497\n",
      "         train       0.01      0.03      0.02       597\n",
      "         truck       0.01      0.01      0.01      1303\n",
      "            tv       0.01      0.00      0.01       837\n",
      "      umbrella       0.00      0.00      0.00      1225\n",
      "          vase       0.01      0.00      0.01       820\n",
      "    wine glass       0.00      0.00      0.00       836\n",
      "         zebra       0.01      0.01      0.01       622\n",
      "\n",
      "      accuracy                           0.13     95894\n",
      "     macro avg       0.01      0.01      0.01     95894\n",
      "  weighted avg       0.08      0.13      0.10     95894\n",
      "\n",
      "Confusion Matrix\n",
      "[[12  2  0 ...  4  0  8]\n",
      " [11  3  1 ...  4  0  6]\n",
      " [20  7  0 ...  5  2 11]\n",
      " ...\n",
      " [12  5  0 ...  3  0  9]\n",
      " [12  6  0 ...  4  0  4]\n",
      " [ 7  7  0 ...  3  1  5]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\xxnab\\OneDrive\\Documents\\GitHub\\FYP\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\xxnab\\OneDrive\\Documents\\GitHub\\FYP\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\xxnab\\OneDrive\\Documents\\GitHub\\FYP\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications import MobileNetV2\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import numpy as np\n",
    "\n",
    "# Paths\n",
    "train_dir = r'C:\\\\Users\\\\xxnab\\\\fiftyone\\\\coco-2017\\\\tensorflow_dataset\\\\train'\n",
    "val_dir = r'C:\\\\Users\\\\xxnab\\\\fiftyone\\\\coco-2017\\\\tensorflow_dataset\\\\validation'\n",
    "test_dir = r'C:\\\\Users\\\\xxnab\\\\fiftyone\\\\coco-2017\\\\tensorflow_dataset\\\\test'\n",
    "\n",
    "# Image data generators\n",
    "train_datagen = ImageDataGenerator(rescale=1./255)\n",
    "val_datagen = ImageDataGenerator(rescale=1./255)\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=(224, 224),\n",
    "    batch_size=32,\n",
    "    class_mode='categorical'\n",
    ")\n",
    "\n",
    "val_generator = val_datagen.flow_from_directory(\n",
    "    val_dir,\n",
    "    target_size=(224, 224),\n",
    "    batch_size=32,\n",
    "    class_mode='categorical'\n",
    ")\n",
    "\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "    test_dir,\n",
    "    target_size=(224, 224),\n",
    "    batch_size=32,\n",
    "    class_mode='categorical'\n",
    ")\n",
    "\n",
    "# Load the MobileNetV2 model\n",
    "base_model = MobileNetV2(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "\n",
    "# Add custom layers on top of the base model\n",
    "x = base_model.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Dense(1024, activation='relu')(x)\n",
    "predictions = Dense(train_generator.num_classes, activation='softmax')(x)\n",
    "\n",
    "# Create the final model\n",
    "model = Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "# Freeze the base model layers\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=Adam(), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Early stopping callback\n",
    "early_stopping = EarlyStopping(monitor='val_accuracy', patience=3, restore_best_weights=True)\n",
    "\n",
    "# Train the model with early stopping\n",
    "model.fit(\n",
    "    train_generator,\n",
    "    epochs=10,\n",
    "    validation_data=val_generator,\n",
    "    callbacks=[early_stopping]\n",
    ")\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "test_loss, test_accuracy = model.evaluate(test_generator)\n",
    "print(f'Test Loss: {test_loss}')\n",
    "print(f'Test Accuracy: {test_accuracy}')\n",
    "\n",
    "# Get true labels and predictions\n",
    "test_generator.reset()\n",
    "Y_pred = model.predict(test_generator)\n",
    "y_pred = np.argmax(Y_pred, axis=1)\n",
    "y_true = test_generator.classes\n",
    "\n",
    "# Classification report\n",
    "print('Classification Report')\n",
    "print(classification_report(y_true, y_pred, target_names=test_generator.class_indices.keys()))\n",
    "\n",
    "# Confusion matrix\n",
    "print('Confusion Matrix')\n",
    "print(confusion_matrix(y_true, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model training for person only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 55593 images belonging to 1 classes.\n",
      "Found 25050 images belonging to 1 classes.\n",
      "Found 25304 images belonging to 1 classes.\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\xxnab\\anaconda3\\Lib\\site-packages\\keras\\src\\ops\\nn.py:545: UserWarning: You are using a softmax over axis -1 of a tensor of shape (None, 1). This axis has size 1. The softmax operation will always return the value 1, which is likely not what you intended. Did you mean to use a sigmoid instead?\n",
      "  warnings.warn(\n",
      "c:\\Users\\xxnab\\anaconda3\\Lib\\site-packages\\keras\\src\\losses\\losses.py:27: SyntaxWarning: In loss categorical_crossentropy, expected y_pred.shape to be (batch_size, num_classes) with num_classes > 1. Received: y_pred.shape=(None, 1). Consider using 'binary_crossentropy' if you only have 2 classes.\n",
      "  return self.fn(y_true, y_pred, **self._fn_kwargs)\n",
      "c:\\Users\\xxnab\\anaconda3\\Lib\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:122: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1738/1738\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m838s\u001b[0m 479ms/step - accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 2/10\n",
      "\u001b[1m1738/1738\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m832s\u001b[0m 478ms/step - accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 3/10\n",
      "\u001b[1m1738/1738\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m804s\u001b[0m 462ms/step - accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 4/10\n",
      "\u001b[1m1738/1738\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m807s\u001b[0m 464ms/step - accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "\u001b[1m791/791\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m254s\u001b[0m 321ms/step - accuracy: 1.0000 - loss: 0.0000e+00\n",
      "Test Loss: 0.0\n",
      "Test Accuracy: 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\xxnab\\anaconda3\\Lib\\site-packages\\keras\\src\\ops\\nn.py:545: UserWarning: You are using a softmax over axis -1 of a tensor of shape (32, 1). This axis has size 1. The softmax operation will always return the value 1, which is likely not what you intended. Did you mean to use a sigmoid instead?\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m791/791\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m251s\u001b[0m 316ms/step\n",
      "Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      person       1.00      1.00      1.00     25304\n",
      "\n",
      "    accuracy                           1.00     25304\n",
      "   macro avg       1.00      1.00      1.00     25304\n",
      "weighted avg       1.00      1.00      1.00     25304\n",
      "\n",
      "Confusion Matrix\n",
      "[[25304]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\xxnab\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:386: UserWarning: A single label was found in 'y_true' and 'y_pred'. For the confusion matrix to have the correct shape, use the 'labels' parameter to pass all known labels.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications import MobileNetV2\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import numpy as np\n",
    "\n",
    "# Paths\n",
    "train_dir = r'C:\\\\Users\\\\xxnab\\\\fiftyone\\\\coco-2017\\\\tensorflow_dataset\\\\train_person'\n",
    "val_dir = r'C:\\\\Users\\\\xxnab\\\\fiftyone\\\\coco-2017\\\\tensorflow_dataset\\\\validation_person'\n",
    "test_dir = r'C:\\\\Users\\\\xxnab\\\\fiftyone\\\\coco-2017\\\\tensorflow_dataset\\\\test_person'\n",
    "\n",
    "# Image data generators\n",
    "train_datagen = ImageDataGenerator(rescale=1./255)\n",
    "val_datagen = ImageDataGenerator(rescale=1./255)\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=(224, 224),\n",
    "    batch_size=32,\n",
    "    class_mode='categorical'\n",
    ")\n",
    "\n",
    "val_generator = val_datagen.flow_from_directory(\n",
    "    val_dir,\n",
    "    target_size=(224, 224),\n",
    "    batch_size=32,\n",
    "    class_mode='categorical'\n",
    ")\n",
    "\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "    test_dir,\n",
    "    target_size=(224, 224),\n",
    "    batch_size=32,\n",
    "    class_mode='categorical'\n",
    ")\n",
    "\n",
    "# Load the MobileNetV2 model\n",
    "base_model = MobileNetV2(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "\n",
    "# Add custom layers on top of the base model\n",
    "x = base_model.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Dense(1024, activation='relu')(x)\n",
    "predictions = Dense(1, activation='softmax')(x)  # Only one class: 'person'\n",
    "\n",
    "# Create the final model\n",
    "model = Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "# Freeze the base model layers\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=Adam(), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Early stopping callback\n",
    "early_stopping = EarlyStopping(monitor='val_accuracy', patience=3, restore_best_weights=True)\n",
    "\n",
    "# Train the model with early stopping\n",
    "model.fit(\n",
    "    train_generator,\n",
    "    epochs=10,\n",
    "    validation_data=val_generator,\n",
    "    callbacks=[early_stopping]\n",
    ")\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "test_loss, test_accuracy = model.evaluate(test_generator)\n",
    "print(f'Test Loss: {test_loss}')\n",
    "print(f'Test Accuracy: {test_accuracy}')\n",
    "\n",
    "# Get true labels and predictions\n",
    "test_generator.reset()\n",
    "Y_pred = model.predict(test_generator)\n",
    "y_pred = np.argmax(Y_pred, axis=1)\n",
    "y_true = test_generator.classes\n",
    "\n",
    "# Classification report\n",
    "print('Classification Report')\n",
    "print(classification_report(y_true, y_pred, target_names=test_generator.class_indices.keys()))\n",
    "\n",
    "# Confusion matrix\n",
    "print('Confusion Matrix')\n",
    "print(confusion_matrix(y_true, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "model.save(r'C:\\\\Users\\\\xxnab\\\\OneDrive\\\\Documents\\\\GitHub\\\\FYP\\\\model\\\\model_person_only.keras')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model training with person and negative samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 167386 images belonging to 2 classes.\n",
      "Found 87825 images belonging to 2 classes.\n",
      "Found 88267 images belonging to 2 classes.\n",
      "Epoch 1/10\n",
      "\u001b[1m5231/5231\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3469s\u001b[0m 662ms/step - accuracy: 0.6351 - loss: 0.6056 - val_accuracy: 0.6506 - val_loss: 0.5833\n",
      "Epoch 2/10\n",
      "\u001b[1m5231/5231\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2577s\u001b[0m 492ms/step - accuracy: 0.6444 - loss: 0.5766 - val_accuracy: 0.6556 - val_loss: 0.5820\n",
      "Epoch 3/10\n",
      "\u001b[1m5231/5231\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2584s\u001b[0m 494ms/step - accuracy: 0.6434 - loss: 0.5742 - val_accuracy: 0.6510 - val_loss: 0.5838\n",
      "Epoch 4/10\n",
      "\u001b[1m5231/5231\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2596s\u001b[0m 496ms/step - accuracy: 0.6426 - loss: 0.5700 - val_accuracy: 0.6615 - val_loss: 0.5728\n",
      "Epoch 5/10\n",
      "\u001b[1m5231/5231\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2600s\u001b[0m 497ms/step - accuracy: 0.6440 - loss: 0.5674 - val_accuracy: 0.6497 - val_loss: 0.5748\n",
      "Epoch 6/10\n",
      "\u001b[1m5231/5231\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2549s\u001b[0m 487ms/step - accuracy: 0.6451 - loss: 0.5643 - val_accuracy: 0.6546 - val_loss: 0.5793\n",
      "Epoch 7/10\n",
      "\u001b[1m5231/5231\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2564s\u001b[0m 490ms/step - accuracy: 0.6475 - loss: 0.5622 - val_accuracy: 0.6553 - val_loss: 0.5725\n",
      "\u001b[1m2759/2759\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1226s\u001b[0m 444ms/step - accuracy: 0.6618 - loss: 0.5736\n",
      "Test Loss: 0.5740429759025574\n",
      "Test Accuracy: 0.6600201725959778\n",
      "\u001b[1m2759/2759\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m917s\u001b[0m 332ms/step\n",
      "Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.59      0.55      0.57     52064\n",
      "      person       0.41      0.45      0.43     36203\n",
      "\n",
      "    accuracy                           0.51     88267\n",
      "   macro avg       0.50      0.50      0.50     88267\n",
      "weighted avg       0.52      0.51      0.51     88267\n",
      "\n",
      "Confusion Matrix\n",
      "[[28634 23430]\n",
      " [19981 16222]]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications import MobileNetV2\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import numpy as np\n",
    "\n",
    "# Paths\n",
    "train_dir = r'C:\\\\Users\\\\xxnab\\\\fiftyone\\\\coco-2017\\\\tensorflow_dataset\\\\train_person'\n",
    "val_dir = r'C:\\\\Users\\\\xxnab\\\\fiftyone\\\\coco-2017\\\\tensorflow_dataset\\\\validation_person'\n",
    "test_dir = r'C:\\\\Users\\\\xxnab\\\\fiftyone\\\\coco-2017\\\\tensorflow_dataset\\\\test_person'\n",
    "\n",
    "# Image data generators\n",
    "train_datagen = ImageDataGenerator(rescale=1./255)\n",
    "val_datagen = ImageDataGenerator(rescale=1./255)\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=(224, 224),\n",
    "    batch_size=32,\n",
    "    class_mode='binary'\n",
    ")\n",
    "\n",
    "val_generator = val_datagen.flow_from_directory(\n",
    "    val_dir,\n",
    "    target_size=(224, 224),\n",
    "    batch_size=32,\n",
    "    class_mode='binary'\n",
    ")\n",
    "\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "    test_dir,\n",
    "    target_size=(224, 224),\n",
    "    batch_size=32,\n",
    "    class_mode='binary'\n",
    ")\n",
    "\n",
    "# Load the MobileNetV2 model\n",
    "base_model = MobileNetV2(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "\n",
    "# Add custom layers on top of the base model\n",
    "x = base_model.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Dense(1024, activation='relu')(x)\n",
    "predictions = Dense(1, activation='sigmoid')(x)  # Binary classification\n",
    "\n",
    "# Create the final model\n",
    "model = Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "# Freeze the base model layers\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=Adam(), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Early stopping callback\n",
    "early_stopping = EarlyStopping(monitor='val_accuracy', patience=3, restore_best_weights=True)\n",
    "\n",
    "# Train the model with early stopping\n",
    "model.fit(\n",
    "    train_generator,\n",
    "    epochs=10,\n",
    "    validation_data=val_generator,\n",
    "    callbacks=[early_stopping]\n",
    ")\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "test_loss, test_accuracy = model.evaluate(test_generator)\n",
    "print(f'Test Loss: {test_loss}')\n",
    "print(f'Test Accuracy: {test_accuracy}')\n",
    "\n",
    "# Get true labels and predictions\n",
    "test_generator.reset()\n",
    "Y_pred = model.predict(test_generator)\n",
    "y_pred = (Y_pred > 0.5).astype(int).flatten()\n",
    "y_true = test_generator.classes\n",
    "\n",
    "# Classification report\n",
    "print('Classification Report')\n",
    "print(classification_report(y_true, y_pred, target_names=['negative', 'person']))\n",
    "\n",
    "# Confusion matrix\n",
    "print('Confusion Matrix')\n",
    "print(confusion_matrix(y_true, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "model.save(r'C:\\\\Users\\\\xxnab\\\\OneDrive\\\\Documents\\\\GitHub\\\\FYP\\\\model\\\\model_person_negative_only.keras')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cuda-gpt",
   "language": "python",
   "name": "cuda"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
